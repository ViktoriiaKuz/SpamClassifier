{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"this code accepts the raw files of spammers and normal users messages and other parameters (such as number of recipients/first and last message sent, number of chats of the specified type, and returns trained model which can classify new users according to their messages and parameters it trains different models, indicates the level of accuracy and save the one which have the best output parameters. The code is built in the way to reduce processing time and \"save to files/pickles\", in order to restart the process from the middle (in case if part of job is already done)\"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import fastparquet\n",
    "import pyarrow.parquet as pq\n",
    "import warnings\n",
    "import spacy\n",
    "import langdetect\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.decomposition import SparsePCA, TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from datetime import datetime\n",
    "from deep_translator import GoogleTranslator\n",
    "from deep_translator.exceptions import NotValidLength\n",
    "from langdetect import LangDetectException\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm\n",
    "from settings import RAW_DATA_PATH\n",
    "from settings import SEPARATED_DATA_PATH\n",
    "from settings import ROOT_PATH\n",
    "from settings import PROCESSED_DATA_PATH\n",
    "from settings import NORMALIZED_DATA_PATH\n",
    "from pandas.errors import SettingWithCopyWarning\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "LABEL_KEY = 'label'\n",
    "CHAT_KEY = \"ad_chat/ad_trigger/user_to_user\"\n",
    "PHRASES_TO_DELETE = [\n",
    "    \"посмотрел(а) ваш номер телефона\",\n",
    "    \"актуально\",\n",
    "    \"просмотрел(а) ваш номер телефона\",\n",
    "    \"ваше объявление\",\n",
    "    \"Name:\",\n",
    "    \"bottom.payload\",\n",
    "    \"Name: , dtype: object\",\n",
    "    \"Name: , Length: , dtype: object\",\n",
    "    \"Name: , dtype: object\",\n",
    "    \"добавил(а) в Избранное\", \"dtype:\", \"object\", \"dtype: object\", \"просмотрел(а) ваш номер телефона\",\n",
    "    \"Length: , dtype: object\", \"Ещё актуально?\", \"n/\", \"name\", \"Name\", \":\"]\n",
    "\n",
    "ROWS_TO_SEPARATE = 50000\n",
    "UNKNOWN_KEY = 'Unknown'\n",
    "RUSSIAN_CODE = 'ru'\n",
    "ENGLISH_CODE = 'en'\n",
    "POLISH_CODE = 'pl'\n",
    "CONVERSATION_TYPE_COL = 'conversartion_type'\n",
    "OWNER_COL = 'owner'\n",
    "RECIPIENT_COL = 'recipient'\n",
    "CHAT_ID_COL = 'chat_id'\n",
    "AD_TRIGGER_COL = '{ad_trigger,'\n",
    "AD_CHAT_COL = '{ad_chat,'\n",
    "USER_TO_USER_COL = '{user_to_user'\n",
    "UPDATED_COL = 'updated'\n",
    "MESSAGES_COL = 'user_messages'\n",
    "DURATION = 'duration'\n",
    "MONGO_ID_COL = '_id'\n",
    "\n",
    "\n",
    "#loading models at this step, helps to speed up the process and no future need to load it every time code meets the language.\n",
    "RUSSIAN_MODEL = spacy.load('ru_core_news_sm')\n",
    "POLISH_MODEL = spacy.load('pl_core_news_sm')\n",
    "ENGLISH_MODEL = spacy.load('en_core_web_sm')\n",
    "\n",
    "dfs_list = []\n",
    "newtext = [\"Free entry\"]\n",
    "header_text = ['owner', 'total_recipients', 'total_chats', 'newest_message', 'oldest_message', 'label', 'text', 'duration', 'chats_per_day', 'recipients_per_day', 'language']\n",
    "header = ['owner', 'total_recipients', 'total_chats', 'newest_message', 'oldest_message', 'label', 'duration', 'chats_per_day', 'recipients_per_day', 'text', 'language']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Ignore the warning about setting a value on a copy of a slice from a DataFrame\n",
    "warnings.filterwarnings('ignore', category=SettingWithCopyWarning)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a794fb16c03e09fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#initital files were too big to be processed in one file, so this function separates code to small and efficient .parquet files\n",
    "def files_separation(file_paths):\n",
    "    marker = os.path.basename(file_paths).split('_')[0]\n",
    "    df = pd.read_csv(file_paths, encoding=\"utf-8-sig\")\n",
    "    list_df = [df[i:i + ROWS_TO_SEPARATE] for i in range(0, len(df), ROWS_TO_SEPARATE)]\n",
    "    for i, df in tqdm(enumerate(list_df)):\n",
    "        df[LABEL_KEY] = marker\n",
    "        df.to_parquet(os.path.join(SEPARATED_DATA_PATH, f\"{marker}_{i}.parquet\"), index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b3ef50dd7da2a38"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this step was specific to data provided\n",
    "def replacing_commas(file_path):\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df.replace({r'[\\n]+': ''}, regex=True).dropna().reset_index(drop=True)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7690648ed7b56226"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "#this two functions parse set[] string from mongo db table column, which looks like \n",
    "# {<user_id>,<chat_type>,<recipient_id>,<owner_id>,<chat_id>}. \n",
    "# according to the type of chat, for example, if the chat was of specified type \"ad_trigger\"\n",
    "# this set sting changes its appearance to \n",
    "# {<user_id>,<chat_type>,<owner_id>,<recipient_id>}\n",
    "# so the code parses it tp the flat table\n",
    "def parse_values(set_string):\n",
    "    values_list = set_string[1:-1].split(' ')\n",
    "\n",
    "    if USER_TO_USER_COL in set_string:\n",
    "        owner = values_list[0]\n",
    "        recipient = values_list[3]\n",
    "        chat_id = 'unknown'\n",
    "    elif any([t in set_string for t in [AD_CHAT_COL, AD_TRIGGER_COL]]):\n",
    "        owner = values_list[0]\n",
    "        recipient = values_list[4]\n",
    "        chat_id = values_list[2]\n",
    "    else:\n",
    "        raise Exception(f\"{set_string} contains unknown value\")\n",
    "\n",
    "    return {\n",
    "        CONVERSATION_TYPE_COL: values_list[1],\n",
    "        OWNER_COL: owner,\n",
    "        RECIPIENT_COL: recipient,\n",
    "        CHAT_ID_COL: chat_id\n",
    "    }\n",
    "\n",
    "def separating_colummns(df):\n",
    "    df[MONGO_ID_COL] = df[MONGO_ID_COL].map(lambda x: parse_values(x))\n",
    "\n",
    "    for key in OWNER_COL, RECIPIENT_COL, CHAT_ID_COL, CONVERSATION_TYPE_COL:\n",
    "        df[key] = df[MONGO_ID_COL].map(lambda x: x[key])\n",
    "\n",
    "    conversation_type_df = pd.get_dummies(df[CONVERSATION_TYPE_COL])\n",
    "    df.drop(columns=[MONGO_ID_COL, CONVERSATION_TYPE_COL], inplace=True)\n",
    "\n",
    "    return pd.concat([df, conversation_type_df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T07:51:37.288586Z",
     "start_time": "2023-09-27T07:51:37.263580700Z"
    }
   },
   "id": "2e5491320b24df53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this function groups all data by owner id, since we have more than one row of data for the user. Also this function transforms dataframes in order to replace \"blocked\", \"regular\" with dummy coding\n",
    "def grouping_by(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df['label'] = df['label'].apply(lambda x: 1 if x == \"blocked\" else 0)\n",
    "    if 'bottom.payload' not in df.columns:\n",
    "        df = df.rename(columns={'user_messages': 'bottom.payload'})\n",
    "\n",
    "    df = (\n",
    "        df.assign(owner_index=df.index)\n",
    "        .groupby(OWNER_COL)\n",
    "        .agg(\n",
    "            label=(LABEL_KEY, \"first\"),\n",
    "            total_recipients=(RECIPIENT_COL, \"count\"),\n",
    "            total_chats=(CHAT_ID_COL, \"count\"),\n",
    "            newest_message=(UPDATED_COL, \"max\"),\n",
    "            oldest_message=(UPDATED_COL, \"min\"),\n",
    "            user_messages=(\"bottom.payload\", lambda x: \"\".join([str(e) for e in x]))\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "    if USER_TO_USER_COL in df:\n",
    "        df['total_user_to_user'] = df.groupby(USER_TO_USER_COL)[\"sum\"]\n",
    "    if AD_CHAT_COL in df:\n",
    "        df['total_ad_chats'] = df.groupby(AD_CHAT_COL)[\"sum\"]\n",
    "    if AD_TRIGGER_COL in df:\n",
    "        df['total_ad_trigger'] = df.groupby(AD_TRIGGER_COL)[\"sum\"]\n",
    "\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3e1be8a3949ef125"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#creating \"duration\" column. Duration will be our \"feature\" for the future analysis. Based on assumptions that normal users and spammers may have different time periods spend with platform \n",
    "def duration_processing(df):\n",
    "    def calculate_duration(first_message_date, last_message_date):\n",
    "        FORMAT = \"%Y-%m-%d %H:%M:%S\"\n",
    "        return datetime.strptime(str(first_message_date), FORMAT) - \\\n",
    "            datetime.strptime(str(last_message_date), FORMAT)\n",
    "\n",
    "    FIRST_MESSAGE = 'newest_message'\n",
    "    LAST_MESSAGE = 'oldest_message'\n",
    "    df['duration'] = list(map(lambda f, l: calculate_duration(f, l), df[FIRST_MESSAGE], df[LAST_MESSAGE]))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "874b38d4109da3e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_total(duration, total):\n",
    "    if duration.total_seconds() > 0:\n",
    "        return total / duration.total_seconds()\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b0a86ef467d5c701"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#another \"feature\" total chats. Based on assumption that total number of chats can be statistically significantly different \n",
    "def total_chats_count(df):\n",
    "    CHATS_TOTAL = 'total_chats'\n",
    "    df['chats_per_day'] = list(map(lambda d, t: calculate_total(d, t), df[DURATION], df[CHATS_TOTAL]))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3312b934ec8af68f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#another \"feature\" total recipients. Based on assumptions that spammers, probably, could have bigger/different number of recipients.\n",
    "def total_recipients_count(df):\n",
    "    RECIPIENTS_TOTAL = 'total_recipients'\n",
    "    df['recipients_per_day'] = list(map(lambda d, t: calculate_total(d, t), df[DURATION], df[RECIPIENTS_TOTAL]))\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10e0aa4d48b834ae"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#since we had a lot of parquet files, i need to concatenate it back in order to save it to one pickle of already processed data\n",
    "def concatenate(df):\n",
    "    dfs_list.append(df)\n",
    "    concatenated_df = dd.concat(dfs_list, ignore_index=True)\n",
    "    with open(\"pickle.pkl\", \"wb\") as f:\n",
    "        pickle.dump(concatenated_df, f)\n",
    "    return concatenated_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "381e78ca8d59d5b4"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "#next three cells demonstrates the code, which make a lemmatization process in a multilingual context. \n",
    "#1. Detect language by the first 15 words in a message \n",
    "#2. If the length is less or the language is not of the most popular in dataset, then mark the language to \"unknown\" and later translate it to english. If this is not possible, left it as unknown.\n",
    "#3. This process is performed in chunks, according to the length of the 'text' and saved to csv's.(here could be parquet's, but it was no performance issues with csv's on this step)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T08:14:51.292435400Z",
     "start_time": "2023-09-27T08:14:51.260423800Z"
    }
   },
   "id": "4193d47159792b58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_single_text(text):\n",
    "    nlp = RUSSIAN_MODEL\n",
    "    if len(text.split()) < 15 or len(text) > 5000:\n",
    "        lang = \"unknown\"\n",
    "        processed_text = text\n",
    "    else:\n",
    "        try:\n",
    "            lang = langdetect.detect(\" \".join(text.split(\" \")[:15]))\n",
    "            if lang == RUSSIAN_CODE:\n",
    "                nlp = RUSSIAN_MODEL\n",
    "            elif lang == POLISH_CODE:\n",
    "                nlp = POLISH_MODEL\n",
    "            elif lang == ENGLISH_CODE:\n",
    "                nlp = ENGLISH_MODEL\n",
    "\n",
    "            processed_text = \" \".join([token.lemma_ for token in nlp(text)])\n",
    "        except NotValidLength as ntl:\n",
    "            lang = \"unknown\"\n",
    "            processed_text = text\n",
    "\n",
    "        except langdetect.lang_detect_exception.LangDetectException:\n",
    "\n",
    "            translator = GoogleTranslator()\n",
    "            translation = translator.translate(text, target='en')\n",
    "            if translation is not None:\n",
    "                lang = 'translated'\n",
    "                nlp = spacy.load('en_core_web_sm')\n",
    "                processed_text = \" \".join([token.lemma_ for token in nlp(translation)])\n",
    "            else:\n",
    "\n",
    "                lang = 'unknown'\n",
    "                processed_text = text\n",
    "\n",
    "    return {\n",
    "        'text': processed_text,\n",
    "        'language': lang\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5f4017340390e22"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def lemmatize(df):\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = header_text[:10]\n",
    "    if len(df['text']) > 1000:\n",
    "        chunk_size = 500\n",
    "        total_rows = len(df['text'])\n",
    "        num_chunks = total_rows // chunk_size\n",
    "\n",
    "        for i in tqdm(range(num_chunks + 1)):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = min((i + 1) * chunk_size, total_rows)\n",
    "            process_chunk_and_save(df[start_idx:end_idx], i)\n",
    "    else:\n",
    "        process_chunk_and_save(df, 1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c2e84f7173edcd5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def process_chunk_and_save(chunk_df, i):\n",
    "    processed_chunk = []\n",
    "\n",
    "    for text in chunk_df['text']:\n",
    "        result = preprocess_single_text(text)\n",
    "        processed_chunk.append(result)\n",
    "\n",
    "    processed_df = pd.DataFrame(processed_chunk)\n",
    "    processed_df = processed_df.reset_index(drop=True)\n",
    "    chunk_df = chunk_df.drop(columns='text')\n",
    "    processed_df = pd.concat([chunk_df, processed_df], axis=1)\n",
    "    print(\"processed_df_len\", len(processed_df))\n",
    "    processed_df.to_csv(os.path.join(PROCESSED_DATA_PATH, f'preprocessed_data_{i}.csv'),\n",
    "                        encoding='utf-8-sig',\n",
    "                        index=False, header=False, errors='ignore')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "15cfad4d6c374f33"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def preprocess_files():\n",
    "    if not os.path.exists(check_data_preprocessed):\n",
    "        file_list = os.listdir(SEPARATED_DATA_PATH)\n",
    "\n",
    "        for i in tqdm(range(len(file_list))):\n",
    "            file_name = file_list[i]\n",
    "            file_path = os.path.join(SEPARATED_DATA_PATH, file_name)\n",
    "            df = replacing_commas(file_path)\n",
    "            df = separating_colummns(df)\n",
    "            df = grouping_by(df)\n",
    "            df = duration_processing(df)\n",
    "            df = total_chats_count(df)\n",
    "            df = total_recipients_count(df)\n",
    "            dfc = concatenate(df)\n",
    "    with open(r\"pickle.pkl\", \"rb\") as f:\n",
    "        dfc = pickle.load(f)\n",
    "\n",
    "    dfc = pd.DataFrame(dfc)\n",
    "    df = lemmatize(dfc)\n",
    "\n",
    "    return df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6175a6c0528a1489"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#since i have \"duration\" column. I no longer need this columns here. Arguably i need to take this function before saving to pickle. \n",
    "def drop_columns(df):\n",
    "    columns_to_drop = ['newest_message', 'oldest_message']\n",
    "    columns_to_drop_existing = []\n",
    "\n",
    "    for col in columns_to_drop:\n",
    "        if col in df.columns:\n",
    "            columns_to_drop_existing.append(col)\n",
    "\n",
    "    if columns_to_drop_existing:\n",
    "        df = df.drop(columns=columns_to_drop_existing)\n",
    "\n",
    "    return df\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e0548d7259feee91"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dropping columns to create dataframe with only numbers\n",
    "#TODO: exclude ownerid\n",
    "def numerical(df):\n",
    "    numerical_df = df.drop(columns=['text', 'label'])\n",
    "    return numerical_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e1c8b04186061ff"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#using MinMaxScaler in order to normalize numerical data to one scale. There are other ways to normalize numerical data: Z-normalize, Log Transform, Box-Cox transformation, Max Abs and other all used in a different scenarios. In scenario like this, simple classification, the coise should be around MinMaxScaler and Z-normalize. Z-normalize assumes normal distribution of a data and helps to fix outliers. I don't want to suggest the normality of the data, so that's why not using Z-normalization.\n",
    "def normalize_numerical(df):\n",
    "    df = df.drop(columns=['duration', 'language', 'owner'])\n",
    "    scaler = MinMaxScaler()\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    dft = scaler.fit_transform(df)\n",
    "    dft = pd.DataFrame(dft, columns=df.columns)\n",
    "    return dft"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-27T08:26:27.444808900Z",
     "start_time": "2023-09-27T08:26:27.418065200Z"
    }
   },
   "id": "b11c203d5f1775dc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#I was interested in a most common words used by different groups of users. \n",
    "def text_df_count(df):\n",
    "    text_spam = df[df['label'] == 'blocked']['text']\n",
    "    text_normal = df[df['label'] == 'regular']['text']\n",
    "\n",
    "    blocked_dict = {}\n",
    "    normal_dict = {}\n",
    "\n",
    "    for text in text_spam:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            blocked_dict[word] = blocked_dict.get(word, 0) + 1\n",
    "\n",
    "    for text in text_normal:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            normal_dict[word] = normal_dict.get(word, 0) + 1\n",
    "\n",
    "    common_words = set(blocked_dict.keys()) & set(normal_dict.keys())\n",
    "    common_words = pd.DataFrame(common_words)\n",
    "\n",
    "    blocked_dict = {word: freq for word, freq in blocked_dict.items() if word not in common_words}\n",
    "    normal_dict = {word: freq for word, freq in normal_dict.items() if word not in common_words}\n",
    "\n",
    "    blocked_df = pd.DataFrame(list(blocked_dict.items()), columns=['word', 'spam_frequency'])\n",
    "    normal_df = pd.DataFrame(list(normal_dict.items()), columns=['word', 'normal_frequency'])\n",
    "\n",
    "    count_df = pd.concat([blocked_df, normal_df], axis=1)\n",
    "    count_df.to_csv('word_frequencies.csv', index=False, encoding='utf-8-sig')\n",
    "    common_words.to_csv('common_words.csv', index=False, encoding='utf-8-sig')\n",
    "    # return common_words"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b403d9ee04cdef9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#vectorizing process of the \"text\" \n",
    "def vectorizing_vectorizer(df):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    vectors = vectorizer.fit_transform((df['text']))\n",
    "    features_df = pd.DataFrame(vectors.todense(), columns=vectorizer.get_feature_names_out())\n",
    "    return features_df\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b29f2ee16a633301"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#, separate and normalize numerical features, perform tfidf on non-numerical data and save to parquet. \n",
    "def prepare_dfs_for_train(df):\n",
    "    def prepare_dfs_for_train(df):\n",
    "    fraction_to_sample = 0.5\n",
    "    df = df.sample(frac=fraction_to_sample, random_state=1)\n",
    "    number_df = label_dummy_coding(drop_columns(df))\n",
    "    numerical_df = numerical(number_df)\n",
    "    normalized_df = normalize_numerical(numerical_df)\n",
    "    tfidf_df = vectorizing_vectorizer(number_df)\n",
    "    features_df = pd.concat([normalized_df, tfidf_df], axis=1, ignore_index=True)\n",
    "    features_path = os.path.join(NORMALIZED_DATA_PATH, f\"normalized_features.parquet\")\n",
    "    lable_df_path = os.path.join(NORMALIZED_DATA_PATH, f\"lable_df.parquet\")\n",
    "    features_df.to_parquet(features_path, compression='gzip')\n",
    "    label_df = number_df.filter([\"id\", \"label\"])\n",
    "    label_df.to_parquet(lable_df_path, compression='gzip')\n",
    "    return features_df, label_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44aae441c78db97e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this piece of code takes only 100 most influential features of the high dimensional (due to tf_idf) datatable. Due to decrease run time\n",
    "def perform_pca(X_sparse, n_components=100):\n",
    "    svd = TruncatedSVD(n_components=n_components)\n",
    "    X_reduced = svd.fit_transform(X_sparse)\n",
    "    return csr_matrix(X_reduced)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e01abb40da74fee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "#This block of code trains models in a loop. The models suggested by the best performers for the spam classification task. Each model is trained 100 times, and than on a mean classification report and matrices the decision of which model to save is made\n",
    "def train_files(data_for_models, lable_df):\n",
    "    X_sparse = csr_matrix(data_for_models)\n",
    "    X_sparse = perform_pca(X_sparse)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_sparse, lable_df.iloc[:, 0], test_size=0.15,\n",
    "                                                        random_state=111)\n",
    "\n",
    "    models_list = {'Logistic Regression': LogisticRegression(), 'Decision Tree': DecisionTreeClassifier(),\n",
    "                   'Random Forest': RandomForestClassifier(), 'SVC': SVC()}\n",
    "    pred_scores_word_vectors = {}\n",
    "    results = {}\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "    n_splits = 100\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True)  # чи треба тут random state\n",
    "\n",
    "    for name, classifier in tqdm(models_list.items()):\n",
    "        accuracy_scores = []\n",
    "        classification_reports = []\n",
    "        confusion_matrices = []\n",
    "        y_preds = []\n",
    "\n",
    "        for train_idx, test_idx in tqdm(skf.split(X_sparse, label_df['label'])):\n",
    "            X_train, X_test = X_sparse[train_idx], X_sparse[test_idx]\n",
    "            y_train, y_test = label_df['label'].iloc[train_idx], label_df['label'].iloc[test_idx]\n",
    "\n",
    "            classifier.fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            y_preds.append(y_pred)\n",
    "\n",
    "            accuracy = accuracy_score(y_test, y_pred)\n",
    "            pred_scores_word_vectors[name] = accuracy\n",
    "            accuracy_scores.append(accuracy)\n",
    "\n",
    "            classification_rep = classification_report(y_test, y_pred,\n",
    "                                                       target_names=['class_0_normal', 'class_1_spammers'])\n",
    "            #results[name]['Classification Report'] = classification_rep\n",
    "            classification_reports.append(classification_rep)\n",
    "\n",
    "            cm = confusion_matrix(y_test, y_pred)\n",
    "            confusion_matrices.append(cm)\n",
    "\n",
    "        mean_y_pred = np.concatenate(y_preds)\n",
    "        mean_accuracy = sum(accuracy_scores) / n_splits\n",
    "        mean_confusion_matrix = np.mean(confusion_matrices, axis=0)\n",
    "        f, ax = plt.subplots(figsize=(5, 5))\n",
    "        sns.heatmap(mean_confusion_matrix, annot=True, linewidths=0.5, linecolor=\"red\", fmt=\".0f\", ax=ax)\n",
    "        plt.xlabel(\"y_pred\")\n",
    "        plt.ylabel(\"y_true\")\n",
    "        plt.title(f\"Confusion Matrix - {name} - {mean_accuracy}\")\n",
    "        plt.savefig(f\"Confusion Matrix - {name} - {mean_accuracy}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        mean_classification_report = classification_report(y_test, y_pred,\n",
    "                                                           target_names=['class_0_normal', 'class_1_spammers'])\n",
    "\n",
    "        report_file_path = f'classification_report_{name}.txt'\n",
    "\n",
    "        # Open the file in write mode and write the classification report content\n",
    "        with open(report_file_path, 'w') as report_file:\n",
    "            report_file.write(mean_classification_report)\n",
    "\n",
    "    best_clf_key = max(pred_scores_word_vectors, key=lambda k: pred_scores_word_vectors[k])\n",
    "    best_clf_value = models_list.get(best_clf_key)\n",
    "    # explainer = shap.Explainer(best_clf_value, X_train)\n",
    "    # shap_values = explainer.shap_values(X_test)\n",
    "    # shap.summary_plot(shap_values, X_test)\n",
    "    # print(shap.summary_plot(shap_values, X_test))\n",
    "    BEST_CLF = best_clf_value\n",
    "    print(best_clf_key, best_clf_value)\n",
    "    filename = f'{BEST_CLF}.pkl'\n",
    "    with open(filename, 'wb') as file:\n",
    "        pickle.dump(BEST_CLF, file)\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "78eaa1fdd2c5dff8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#This block of code transforms \"normalized_features\"parquet to dataframe and separates it to \"fearutes\" abd \"lable\" df\n",
    "def prepare_data_for_train(dataframe):\n",
    "    features_df, label_df = prepare_dfs_for_train(dataframe)\n",
    "    features_path = os.path.join(ROOT_PATH, f\"normalized_features.parquet\")\n",
    "    table = pq.read_table(\"normalized_features.parquet\")\n",
    "    features_df = table.to_pandas()\n",
    "    return features_df, label_df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "572b16d160af1216"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#this code start the program in an iterative way\n",
    "#1.It checks whether the specified files are already existed, and if so or some of them so, the code starts from the block of code which produced first not existed file/info and continue to run using the existed data\n",
    "#2. First block separated initial files \n",
    "#3. Second block processes files (perform all initial cleaning and transformations.\n",
    "#4. Third block takes normalized data, concatenate it than splits it to train,test data, trains, tests and creates matrices and  \n",
    "if __name__ == '__main__':\n",
    "    files_list = []\n",
    "    dataframes = pd.DataFrame()\n",
    "    check_raw_data_separated = os.path.join(SEPARATED_DATA_PATH, \"blocked_0.parquet\")\n",
    "    check_data_preprocessed = os.path.join(ROOT_PATH, f\"pickle.pkl\")\n",
    "    check_data_lemmatized = os.path.join(PROCESSED_DATA_PATH, \"preprocessed_data_1.csv\")\n",
    "    check_data_normalized = os.path.join(ROOT_PATH, \"lable_df.parquet\")\n",
    "    check_data_trained = os.path.join(PROCESSED_DATA_PATH, f\"*.png\")\n",
    "\n",
    "    if not os.path.exists(check_raw_data_separated):\n",
    "        file_names_list = [\"blocked_user.csv\", \"regular_user.csv\"]\n",
    "        list_of_all = []\n",
    "        for file_name in file_names_list:\n",
    "            file_path = os.path.join(RAW_DATA_PATH, file_name)\n",
    "            list_of_all.append(file_path)\n",
    "            files_separation(file_path)\n",
    "\n",
    "    if not os.path.exists(check_data_lemmatized):\n",
    "        preprocess_files()\n",
    "\n",
    "    if not os.path.exists(check_data_trained):\n",
    "        if not os.path.exists(check_data_normalized):\n",
    "            file_list = os.listdir(PROCESSED_DATA_PATH)\n",
    "            print(len(file_list))\n",
    "            dataframes = pd.DataFrame()\n",
    "            for i in tqdm(range(len(file_list))):\n",
    "                file_name = file_list[i]\n",
    "                file_path = os.path.join(PROCESSED_DATA_PATH, file_name)\n",
    "                df = pd.read_csv(file_path, encoding='utf=8=sig')\n",
    "                df.columns = [\"owner\", \"label\", \"total_recipients\", \"total_chats\", \"newest_message\", \"oldest_message\",\n",
    "                              \"duration\", 'chats_per_day', 'recipients_per_day', \"text\", \"language\"]\n",
    "                dataframes = pd.concat([dataframes, df], axis=0, ignore_index=True)\n",
    "                dataframes.columns = [\"owner\", \"label\", \"total_recipients\", \"total_chats\", \"newest_message\",\n",
    "                                      \"oldest_message\", \"duration\", 'chats_per_day', 'recipients_per_day', \"text\",\n",
    "                                      \"language\"]\n",
    "             \n",
    "            features_df, label_df = prepare_data_for_train(dataframes)\n",
    "            trained_data = train_files(features_df, label_df)\n",
    "        else:\n",
    "            features_path = os.path.join(ROOT_PATH, \"normalized_features.parquet\")\n",
    "            pf = fastparquet.ParquetFile(\"normalized_features.parquet\")\n",
    "            table = pq.read_table(\"normalized_features.parquet\")\n",
    "            features_df = table.to_pandas()\n",
    "            # features_df = fastparquet.ParquetFile(\"normalized_features.parquet\").to_pandas()\n",
    "            label_df_path = os.path.join(ROOT_PATH, \"label_df.parquet\")\n",
    "            label_df = fastparquet.ParquetFile(\"label_df.parquet\").to_pandas()\n",
    "            trained_data = train_files(features_df, label_df)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "db9b7737c7c4cebe"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
